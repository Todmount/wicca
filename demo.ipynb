{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**License:** This notebook is part of WICCA and is licensed under the [GNU GPL v3.0](./LICENSE).\n",
    "Copyright © 2025 Andrii Lesniak.\n",
    "\n",
    "---"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# WICCA: Wavelet-based Image Compression & Classification Analysis\n",
    "\n",
    "WICCA is a research framework exploring the impact of wavelet compression on image classification performance.\n",
    "The current goal is to assess whether reducing image size via Haar wavelet transformation retains enough information for accurate classification.\n",
    "\n",
    "### Objectives\n",
    "- Apply Haar wavelet compression to large images (>2K resolution).\n",
    "- Evaluate classification performance on both original and compressed images.\n",
    "- Compare results across multiple pre-trained models.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Set TF to use CPU globally,\n",
    "My device is quite old and doesn't support CUDA (anymore). I decided that it would be easier to just ignore GPU.\n",
    "<b>Run this snippet only if you got CUDA compatibility issues.</b>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import os\n",
    "# # Hide GPUs from TF\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "#\n",
    "# import tensorflow as tf\n",
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "#\n",
    "# # Now everything defaults to CPU:0\n",
    "# print(tf.config.list_physical_devices(\"GPU\"))\n",
    "# print(\"GPUs visible to TF:\", tf.config.list_physical_devices(\"GPU\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports"
  },
  {
   "metadata": {
    "executionInfo": {
     "elapsed": 1385,
     "status": "ok",
     "timestamp": 1741288903629,
     "user": {
      "displayName": "Andrii Lesniak",
      "userId": "11355835527617428667"
     },
     "user_tz": -120
    },
    "id": "77mx3KSY1QnF"
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.applications as apps\n",
    "from pathlib import Path\n",
    "\n",
    "import wicca.visualization as viz\n",
    "import wicca.result_manager as rmgr\n",
    "from wicca.data_loader import load_image, load_models\n",
    "from wicca.wavelet_coder import HaarCoder\n",
    "from wicca.classifying_tools import ClassifierProcessor\n",
    "from wicca.config.constants import SIM_CLASSES_PERC, SIM_BEST_CLASS, RESULTS_FOLDER\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.float_format', '{:.5f}'.format)\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0=DEBUG, 1=INFO, 2=WARNING, 3=ERROR"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmHpxs1eFFGj"
   },
   "source": [
    "## Models\n",
    "Let's load our models using [keras.apps](https://keras.io/api/applications/).\n",
    "\n",
    "To load image classification models, provide a dictionary that maps model names to their classes\n",
    "- Keys: Descriptive model names (strings)\n",
    "- Values: Either a model class reference OR a tuple of (model class, configuration dict)\n",
    "- Use tuples for models requiring non-standard input shapes (e.g., 299×299 instead of 224×224)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "models_dict = {\n",
    "    # Mobile Networks (standard 224×224)\n",
    "    'MobileNetV2': apps.mobilenet_v2.MobileNetV2,\n",
    "\n",
    "    # VGG family (standard 224×224)\n",
    "    'VGG16': apps.vgg16.VGG16,\n",
    "    'VGG19': apps.vgg19.VGG19,\n",
    "\n",
    "    # ResNet family (standard 224×224)\n",
    "    'ResNet50': apps.resnet50.ResNet50,\n",
    "    'ResNet101': apps.resnet.ResNet101,\n",
    "\n",
    "    # DenseNet family (standard 224×224)\n",
    "    'DenseNet169': apps.densenet.DenseNet169,\n",
    "    'DenseNet201': apps.densenet.DenseNet201,\n",
    "\n",
    "    # NasNet family (custom shapes)\n",
    "    'NASNetMobile': apps.nasnet.NASNetMobile,  # Uses 224×224\n",
    "    'NASNetLarge': (apps.nasnet.NASNetLarge, {'shape': (331, 331)}),\n",
    "\n",
    "    # Inception family (custom shapes)\n",
    "    'InceptionV3': (apps.inception_v3.InceptionV3, {'shape': (299, 299)}),\n",
    "    'InceptionResNetV2': (apps.inception_resnet_v2.InceptionResNetV2, {'shape': (299, 299)}),\n",
    "\n",
    "    # EfficientNet family (custom shapes)\n",
    "    'EfficientNetB0': apps.efficientnet.EfficientNetB0,\n",
    "    'EfficientNetB1': (apps.efficientnet.EfficientNetB1, {'shape': (240, 240)}),\n",
    "\n",
    "    # Xception (custom shape)\n",
    "    'Xception': (apps.xception.Xception, {'shape': (299, 299)}),\n",
    "}\n",
    "\n",
    "models_dict_light = {\n",
    "    # Mobile Networks (standard 224×224)\n",
    "    'MobileNetV2': apps.mobilenet_v2.MobileNetV2,\n",
    "    # VGG family (standard 224×224)\n",
    "    'VGG16': apps.vgg16.VGG16,\n",
    "    # EfficientNet family (custom shapes)\n",
    "    'EfficientNetB1': (apps.efficientnet.EfficientNetB1, {'shape': (240, 240)}),\n",
    "    # Xception (custom shape)\n",
    "    'Xception': (apps.xception.Xception, {'shape': (299, 299)}),\n",
    "}\n",
    "\n",
    "classifiers = load_models(models_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMzkD3utFT-J"
   },
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample image"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1741288947431,
     "user": {
      "displayName": "Andrii Lesniak",
      "userId": "11355835527617428667"
     },
     "user_tz": -120
    },
    "id": "7o9yADjLHqHn",
    "outputId": "f7ed33e2-93ad-4b09-9bb1-4a285fc31adb"
   },
   "source": [
    "path = Path('data/originals')\n",
    "list_dir = [f.name for f in path.iterdir()]\n",
    "\n",
    "idx = list_dir[4]\n",
    "sample = load_image(f'{path}/{idx}')\n",
    "\n",
    "plt.imshow(sample)\n",
    "plt.axis('off')\n",
    "plt.title(f'{idx}, shape: {sample.shape}')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCcJak9mKdGA"
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start out analysis we need to define a processor.\n",
    "The `ClassifierProcessor` is designed to handle large-image datasets efficiently. It can process images in parallel, save results automatically, and provide visualization tools to compare original images with their classified icons.\n",
    "The class takes care of all preprocessing required by each model, manages the classification workflow, and organizes results in a structured format for easy analysis. After processing, you'll find detailed classification results in your specified results folder, ready for further analysis or visualization.\n",
    "\n",
    "\n",
    "There is some validation for depth input, but it is highly recommended to pass only in specified below formats.\n",
    "\n",
    "**Please be aware, that process might take quite a while for large datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ClassifierProcessor supports a variety of depth inputs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "depth = 5\n",
    "depth_tuple = (3, 4)\n",
    "depth_range = range(2,7)\n",
    "depth_list = [2, 6]\n",
    "depth_tuple_list = [(3, 4), (4, 5), (5, 6)] # would fail"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, it tries to save results into the \"results\" folder in the project core folder. You can change this folder by defining a new path using pathlib or just providing it as str, and passing it as an argument"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_folder = 'data/originals'\n",
    "\n",
    "res_folder_path = Path('C:/101AwesomeProject/results')\n",
    "res_folder_str = 'C:/101AwesomeProject/results'\n",
    "\n",
    "res_default = RESULTS_FOLDER"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single depth"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "processor = ClassifierProcessor(\n",
    "    data_folder='data/originals',  # Set a directory containing images to process\n",
    "    wavelet_coder=HaarCoder(),  # Set our wavelet\n",
    "    transform_depth=range(1,6),  # Set the depth of transforming; accepts int | range | list\n",
    "    top_classes=5,  # Set top classes for comparison\n",
    "    interpolation=cv2.INTER_AREA,  # Set an interpolation method\n",
    "    results_folder='results',  # Set the results folder\n",
    "    log_info=True,  # Print input-related info; enabled by default\n",
    "    batch_size=30 # Set size of image batch for classifier\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "General use of this framework to classify a batch of classifiers:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processor.process_classifiers(classifiers, timeout=3600)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though, you can just pass single a dict with classifier"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# single_classifier = load_models(single_model)\n",
    "# processor.process_classifiers(single_classifier)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you could access single model from already loaded. However, some rules should be followed.\n",
    "You should use following format\n",
    "\n",
    "`process_single_classifier(\"model_name_you_specified\", classfiers_dict[\"model_name_you_specified\"], timeout) `"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# processor.process_single_classifier(\"MobileNetV2\", classifiers[\"MobileNetV2\"], timeout=3600)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Results presentation\n",
    "Let's have a look at our results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JILTW4t4TML6"
   },
   "source": [
    "MobileNetV2"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rmgr.load_summary_results(\n",
    "    results_folder=res_default,\n",
    "    classifier_name='MobileNetV2',\n",
    "    depth=5\n",
    ")                           # We can call results either by loading csv or\n",
    "# results['MobileNetV2']    # By directly looking into \"results\" dataframe"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9ZwHYCuTUFR"
   },
   "source": [
    "EfficientNetB0"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rmgr.load_summary_results(\n",
    "    results_folder=res_default,\n",
    "    classifier_name='EfficientNetB0',\n",
    "    depth = 5\n",
    ")\n",
    "# results['VGG16']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgPE3BDzZGzx"
   },
   "source": [
    "#### Compare classifiers by single depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract values for some visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "comparison = rmgr.compare_summaries(res_default, classifiers, 5, 'mean')\n",
    "names, similar_classes_pct = rmgr.extract_from_comparison(comparison, 'similar classes (%)')\n",
    "names, similar_best_class = rmgr.extract_from_comparison(comparison, 'similar best class')\n",
    "# similar_best_class = similar_best_class * 100"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNtHD0wicWhD"
   },
   "source": [
    "### Similar classes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "l93Z_N3NY0kx",
    "outputId": "aa5b64c4-6ee0-4b43-86bc-9742a8a43c98"
   },
   "source": [
    "viz.plot_metric_radar(names, similar_classes_pct, 'Best 5 Classes Similarity', min_value=75, max_value=95)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5fvo0V8cq5b"
   },
   "source": [
    "### The best class similarity"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "_AzjP7XsZuTJ",
    "outputId": "484d62cf-5f5f-4510-d179-cb141c9f641e"
   },
   "source": [
    "viz.plot_metric_radar(names, similar_best_class, \"Best Class Similarity\", min_value=75, max_value=95)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "viz.plot_compare_metrics(names, similar_classes_pct, similar_best_class)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Results comparison for different depths\n",
    "In this block we compare classifying performance by multiple classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how different depth of transformation affect our images."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "viz.show_image_vs_icon(sample, range(1,6), HaarCoder())",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "viz.show_icon_on_image(\n",
    "    image=sample,\n",
    "    depth_value=range(1,6),\n",
    "    coder=HaarCoder(),\n",
    "    border_width=5,\n",
    "    border_color=(0, 0, 0),\n",
    "    figsize=(12,7)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A heatmap for visualization similar classes cases by models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x = rmgr.compare_summaries(res_default, classifiers, depth_range, \"mean\")\n",
    "viz.visualize_comparison(x, SIM_CLASSES_PERC, title=\"Best 5 Classes Similarity Heatmap\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for the best class cases"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "viz.visualize_comparison(x, SIM_BEST_CLASS, title=\"Best Class Similarity Heatmap\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
